{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289cc4b2",
   "metadata": {},
   "source": [
    "----------------\n",
    "Har Har Maha Dev\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2ae70",
   "metadata": {},
   "source": [
    "> Reousrce - https://cloudchamp.notion.site/How-I-Passed-my-CKA-Exam-55fef633b454438aadc54a7261312ec9\n",
    "> Cmds list - https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#create\n",
    "\n",
    "# Resources:\n",
    "\n",
    "- Kubernetes official Doc: https://kubernetes.io/docs/home/\n",
    "- CKA Mock Practice Test by KodeKloud: https://kodekloud.com/courses/ultimate-certified-kubernetes-administrator-cka-mock-exam/\n",
    "- [Killer.sh](http://Killer.sh) Practice test: https://killer.sh/cka\n",
    "- Killer Coda Scenarios: https://killercoda.com/playgrounds/scenario/cka\n",
    "\n",
    "### TOPICS to absolutely know before sitting the exam\n",
    "\n",
    "1. [ETCD Backup & Restore](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)\n",
    "2. [Cluster Upgrade](https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/)\n",
    "3. [Working with PV and PVC](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/)\n",
    "4. [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n",
    "5. [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)\n",
    "6. [Jsonpath & Custom columns](https://kubernetes.io/docs/reference/kubectl/quick-reference/)\n",
    "7. [Monitoring/Logging Pods and nodes](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_top/)\n",
    "8. VIM & Nano\n",
    "9. [Service Accounts, Role and Role Bindings](https://kubernetes.io/docs/reference/access-authn-authz/authorization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98ab30",
   "metadata": {},
   "source": [
    "171458106\n",
    "\n",
    "‚ò∏Ô∏è KodeKlode Notes: \n",
    "https://notes.kodekloud.com/docs/CKA-Certification-Course-Certified-Kubernetes-Administrator/Introduction/Course-Introduction\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è Basic Kubectl cmds. \n",
    "\n",
    "  - kubectl cluster -info\n",
    "  - kubectl get nodes\n",
    "  - kubectl get nodes -o wide (to get more details info about the pod)\n",
    "  - kubectl get pods\n",
    "  - kubectl get pods -o wide (to get more details info about the pod)\n",
    "\n",
    "===================================================\n",
    "  \n",
    "- ‚ò∏Ô∏è To run an image using k8.                                         \n",
    "ex. kubectl run nginx --image=nginx \n",
    "\n",
    "- ‚ò∏Ô∏è To get details information about a POD                            \n",
    "ex. kubectl describe pod ngnix\n",
    "\n",
    "- ‚ò∏Ô∏è To delete pods with kubeclt                                       \n",
    "ex. kubectl delete pod webapp\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è minikube\n",
    "\n",
    "- kubectl get deployments\n",
    "- kubectl delete services hello-minikube(service_name)\n",
    "- kubectl delete deployment hello-minikube(service_name)\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è A Pod is a group of one or more application containers (such as Docker) and includes shared storage (volumes), IP address and information about how to run them. \n",
    "  A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster.\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è PODS are the smallest object that we can create in k8. Each POD gets it own internal ip within the k8 cluster.\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è We do not add additions containers in an exixting POD to scale up instead, we create new POD to scale up and when the nodes capacity in full we can create new nodes.\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è 127.0.0.1 is local host.\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è In k8 configuration/defination file these four are the required top/root level properties.\n",
    "\n",
    "ex.\n",
    "\n",
    "apiversion:\n",
    "\n",
    "kind:\n",
    "\n",
    "metadata:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spec:\n",
    "\n",
    "\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "ex. \n",
    "\n",
    "\n",
    "\"apiVersion: batch/v1\n",
    "kind: Job/Pod\n",
    "metadata:\n",
    "  name: hello\n",
    "spec:\n",
    "  template:\n",
    "\n",
    "    * This is the pod template\n",
    "\n",
    "      containers:\n",
    "        - name: hello\n",
    "          image: busybox:1.28\n",
    "          command: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\n",
    "      restartPolicy: OnFailure\n",
    "\n",
    "    * he pod template ends here\"\n",
    "\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "\n",
    "ex.\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: postgres\n",
    "  labels:\n",
    "    tier: db-tier\n",
    "spec:\n",
    "  containers:\n",
    "    - name: postgres\n",
    "      image: postgres\n",
    "      env:\n",
    "        - name: POSTGRES_PASSWORD\n",
    "          value: mysecretpassword\n",
    "\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è To create using kubectl via .yaml file                                \n",
    "- kubectl create -f pod-defination.yml(file_name)\n",
    "- kubectl apply -f pod-defination.yml(file_name)\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è To get the number of PODS created by replicationcontroller            \n",
    "- kubectl get replicationcontroller                 (old way)\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è \"selector\" property in k8 defination file is one major diff btw replication controller and replica set.\n",
    "\n",
    "‚ò∏Ô∏è To create replicaset                                                   \n",
    "- kubectl create ‚Äìf replicaset-definition.yml\n",
    "\n",
    "‚ò∏Ô∏è To get the number of PODS created by replicaset.                       \n",
    "- kubectl get replicaset\n",
    "\n",
    "‚ò∏Ô∏è To delete replica set                                                  \n",
    "- kubectl delete replicaset myapp-replicaset  *Also deletes all underlying PODs\n",
    "\n",
    "‚ò∏Ô∏è To scale replicaset                                                    \n",
    "- kubectl replace -f replicaset-definition.yml\n",
    "- kubectl scale -‚Äìreplicas=6 ‚Äìf replicaset-definition.ym\n",
    "- kubectl scale -‚Äìreplicas=6 replicaset myapp-replicaset(name of replica set)\n",
    "- kubectl scale replicaset set_name --replicas=2\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è To edit replica set file without making changes to defination file     \n",
    "- kubectl edit replicaset my-replicaset(name_of_replicaset)\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è To know more information about replicaset                             \n",
    "- kubectl describe replicaset my-replicaset(replicaset_name)\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Replicaset can be used to monitor existing PODS if they are already created.\n",
    "\n",
    "\n",
    "\n",
    "*** ReplicaSet yml file ***\n",
    "\n",
    "apiVersion: apps/v1 \n",
    "kind: ReplicaSet \n",
    "metadata:\n",
    "  name: myapp-replicaset\n",
    "  labels:\n",
    "    app: mywebsite\n",
    "    type: front-end\n",
    "spec:\n",
    "  template: \n",
    "    metadata:\n",
    "      name: myapp-pod\n",
    "      labels:\n",
    "        app: myapp\n",
    "        type: front-end\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: nginx-container\n",
    "          image: nginx\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      type: front-end\n",
    "  replicas: 4\n",
    "\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è To create deployment use cmd.                                      \n",
    "- kubectl create ‚Äìf deployment-definition.yml\n",
    "\n",
    "‚ò∏Ô∏è To record changes use cmd.                                          \n",
    "- kubectl create ‚Äìf deployment-definition.yml --record\n",
    "\n",
    "‚ò∏Ô∏è To see the deployment use cmd.                              \n",
    "- kubectl get deployment\n",
    "\n",
    "‚ò∏Ô∏è To see all at once what is being created by deployment file.\n",
    "- kubectl get all\n",
    "\n",
    "‚ò∏Ô∏è To update deployment\n",
    "- kubectl apply ‚Äìf deployment-definition.yml                          or\n",
    "- kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1                              \n",
    "\n",
    "‚ò∏Ô∏è To edit deployment file without making changes to defination file.\n",
    "- kubectl edit deployment myapp-deployment(name_of_deployment)\n",
    "\n",
    "‚ò∏Ô∏è To check rollout cmd.\n",
    "- kubectl rollout status deployment/myapp-deployment\n",
    "\n",
    "‚ò∏Ô∏è To check rollout history\n",
    "- kubectl rollout history deployment/myapp-deployment\n",
    "\n",
    "‚ò∏Ô∏è To delete deployment use cmd.\n",
    "- kubectl delete deployment myapp-deployment\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è \"Rolling update\" is the default deployment strategy.\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è To rollback to precious version use cmd.\n",
    "- kubectl rollout undo deployment/myapp-deployment\n",
    "\n",
    "\n",
    "ex.\n",
    "\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: frontend\n",
    "  labels:\n",
    "    app: mywebsite\n",
    "    tier: frontend\n",
    "spec:\n",
    "  replicas: 4\n",
    "  template:\n",
    "    metadata:\n",
    "      name: myapp-pod\n",
    "      labels:\n",
    "        app: myapp\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: nginx\n",
    "          image: nginx\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: myapp\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏è In K8 IP address is assigned to each POD, unlike in docker world where IP address is always assigned to a docker container.\n",
    "\n",
    "‚ò∏Ô∏è k8 Services types.\n",
    "\n",
    " 1. Node port service\n",
    " 2. cluster IP service  (is the default service launched if we do not specifiy in defination .yaml file)\n",
    " 3. Loadbalancer service\n",
    "\n",
    "\n",
    "\n",
    "**** Nord port service. defenation yaml file ****\n",
    "\n",
    "\n",
    "ex. \n",
    "\n",
    "\n",
    "apiVersion: v1 \n",
    "kind: Service\n",
    "metadata:\n",
    "  name: myapp-service\n",
    "\n",
    "spec:\n",
    "  type: NodePort\n",
    "  ports:\n",
    "    - targetPort: 80\n",
    "      port: 80\n",
    "      nodePort: 30008\n",
    "  selector:\n",
    "    app: myapp\n",
    "    type: front-end\n",
    "\n",
    "\n",
    "\n",
    "**** Nord port service. defenation yaml file ****\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Nord port value can be btw 30000 to 32767.\n",
    "\n",
    "‚ò∏Ô∏è To check the services create after you run yaml defination file.     \n",
    "- kubectl get services  or kubectl get svc\n",
    "\n",
    "‚ò∏Ô∏è To get detailed info about services.                                 \n",
    "- kubectl describe  svc\n",
    "\n",
    "‚ò∏Ô∏è To find the url of service use cmd.     \n",
    "- minikube service voting-service(name of services) --url\n",
    "\n",
    "\n",
    "\n",
    "***** cluster IP service defenation yaml file *****\n",
    "\n",
    "apiVersion: v1 \n",
    "kind: Service \n",
    "metadata:\n",
    "  name: back-end\n",
    "\n",
    "spec:\n",
    "  type: ClusterIP\n",
    "  ports:\n",
    "    - targetPort: 80\n",
    "      port: 80\n",
    "  selector:\n",
    "    app: myapp\n",
    "    type: front-end\n",
    "    \n",
    "\n",
    "***** cluster IP service defenation yaml file *****\n",
    "\n",
    "\n",
    "\n",
    "***** Loadbalancer service defenation yaml file *****\n",
    "\n",
    "This is supported in cloud platform ex. azre, aws, gcp\n",
    "\n",
    "\n",
    "apiVersion: v1 \n",
    "kind: Service \n",
    "metadata:\n",
    "  name: front-end\n",
    "spec:\n",
    "  type: LoadBalancer \n",
    "  ports:\n",
    "    - targetPort: 80\n",
    "      port: 80\n",
    "  selector:\n",
    "    app: myapp\n",
    "    type: front-end\n",
    "\n",
    "\n",
    "***** Loadbalancer IP service defenation yaml file *****\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Default port for redis image  6379\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Default port for postgres image  5432\n",
    "\n",
    "==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f7fbb",
   "metadata": {},
   "source": [
    "\n",
    "22/10/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Lazy pulling\n",
    "\n",
    "In the context of Kubernetes (k8s) and containerd, lazy pulling (also known as on-demand image loading) is an advanced \n",
    "technique where a container is started before its entire image has been downloaded.\n",
    "\n",
    "Instead of pulling the full image (which could be hundreds of MB or several GB), the runtime only pulls the \n",
    "specific files or blocks of the image data as the container needs them.\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è etcd svc listins (advertise-client-urls) on port 2379 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c38d0",
   "metadata": {},
   "source": [
    "28/10/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è kube command to get to know images of all pods running\n",
    "\n",
    "kubectl get pods -A --field-selector=status.phase=Running -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,IMAGE:spec.containers[*].image\n",
    "\n",
    "‚ò∏Ô∏è kube command to get to know Which nodes are these pods placed on?\n",
    "\n",
    "- kubectl get pods -o wide -A\n",
    "- kubectl get pods -A -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,NODE:spec.nodeName\n",
    "\n",
    "‚ò∏Ô∏è Replication controller also helps in load balancing and scaling.\n",
    "\n",
    "‚ò∏Ô∏è replicationcontroller is older technology replaced by ReplicaSet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e145e",
   "metadata": {},
   "source": [
    "\n",
    "29/10/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Delete the two created ReplicaSets - replicaset-1 and replicaset-2\n",
    "- kubectl delete replicasets replicaset-1 replicaset-2\n",
    "\n",
    "‚ò∏Ô∏è Scale the ReplicaSet to 5 PODs.\n",
    "- kubectl scale replicaset <your-replicaset-name> --replicas=5\n",
    "\n",
    "or edit the replicaset file and use below command.\n",
    "- kubectl replace -f replicaset-definition.yml\n",
    "\n",
    "‚ò∏Ô∏è To see all at once deployments, replicaset, pods\n",
    "- kubectl get all\n",
    "\n",
    "===================================================\n",
    "\n",
    "‚ò∏Ô∏èCertification Tip!\n",
    "\n",
    "As you might have seen already, it is a bit difficult to create and edit YAML files. \n",
    "Especially in the CLI. During the exam, you might find it difficult to copy and paste YAML files from browser to terminal. \n",
    "\n",
    "Using the kubectl run command can help in generating a YAML template. And sometimes, you can even get away with just the \n",
    "kubectl run command without having to create a YAML file at all. \n",
    "\n",
    "For example, if you were asked to create a pod or deployment with specific name and image you can simply run the kubectl run command.\n",
    "\n",
    "Use the below set of commands and try the previous practice tests again, but this time try to use the below commands instead of YAML files. Try to use these as much as you can going forward in all exercises\n",
    "\n",
    "Reference (Bookmark this page for exam. It will be very handy):\n",
    "https://kubernetes.io/docs/reference/kubectl/conventions/\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Create an NGINX Pod\n",
    "kubectl run nginx --image=nginx\n",
    "\n",
    "‚ò∏Ô∏è Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)               (-o It stands for --output yaml.)\n",
    "- kubectl run nginx --image=nginx --dry-run=client -o yaml\n",
    "\n",
    "‚ò∏Ô∏è Create a deployment\n",
    "- kubectl create deployment --image=nginx nginx\n",
    "\n",
    "‚ò∏Ô∏è Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)\n",
    "- kubectl create deployment --image=nginx nginx --dry-run=client -o yaml\n",
    "\n",
    "‚ò∏Ô∏è Generate Deployment YAML file (-o yaml). Don‚Äôt create it(‚Äìdry-run) and save it to a file.\n",
    "- kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml\n",
    "\n",
    "‚ò∏Ô∏è Make necessary changes to the file (for example, adding more replicas) and then create the deployment.\n",
    "- kubectl create -f nginx-deployment.yaml\n",
    "\n",
    "OR\n",
    "\n",
    "‚ò∏Ô∏è In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.\n",
    "- kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml\n",
    "\n",
    "‚ò∏Ô∏è Service\n",
    "Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379\n",
    "- kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml                  or\n",
    "\n",
    "‚ò∏Ô∏è kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml \n",
    "(This will not use the pods labels as selectors, instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:\n",
    "- kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml                     or\n",
    "- kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml\n",
    "\n",
    "\n",
    "===================================================\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è what does \"READY\" Column show in this get deployments output? No of replicasets ready and desired?\n",
    "\n",
    "kubectl get deployments\n",
    "NAME                  READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "frontend-deployment   0/4     4            0           3s\n",
    "\n",
    "- The \"READY\" column for a Deployment shows the ratio of ready pods to desired pods.\n",
    "- The \"UP-TO-DATE: column shows the number of pods that have been updated to match the latest version of your deployment's template.\n",
    "\n",
    "‚ò∏Ô∏è Here also\n",
    "kubectl get replicasets\n",
    "NAME                           DESIRED   CURRENT   READY   AGE\n",
    "frontend-deployment-cd6b557c   4         4         0       2m28s\n",
    "\n",
    "- This also shoes desired, current and ready number of pods\n",
    "\n",
    "‚ò∏Ô∏è Here it shows number of ready container/desired containers.\n",
    "\n",
    "kubectl get pods\n",
    "\n",
    "\n",
    "NAME                                 READY   STATUS             RESTARTS   AGE\n",
    "frontend-deployment-cd6b557c-6zsfw   0/1     ImagePullBackOff   0          6m33s\n",
    "\n",
    "- This pod is supposed to have 1 container, but 0 of them are currently ready.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33d7ad",
   "metadata": {},
   "source": [
    "30/10/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è To get namespace of all pods at once.\n",
    "- kubectl get pods --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de581ffc",
   "metadata": {},
   "source": [
    "03/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è To get to know about the resource you want to deploy.\n",
    "- kubectl api-resource\n",
    "\n",
    "‚ò∏Ô∏èTo get in details info about that resource.\n",
    "- kubectl explain pods \n",
    "- kubectl explain pods.spec\n",
    "- kubectl explain pods --recursive\n",
    "\n",
    "‚ò∏Ô∏è what should be preferred kubectl apply or kubectl create?\n",
    "\n",
    "- You should always prefer kubectl apply. It's the modern, standard way to manage Kubernetes resources, while kubectl create is an older, more limited command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ededb9",
   "metadata": {},
   "source": [
    "\n",
    "03/11/2025\n",
    "----------\n",
    "\n",
    "# Use imperative commands.\n",
    "\n",
    "‚ò∏Ô∏è Create a service named redis-service to expose the existing redis pod within the cluster on port 6379.\n",
    "- kubectl expose pod redis --port=6379 --name=redis-service --type=ClusterIP\n",
    "\n",
    "‚ò∏Ô∏è Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.\n",
    "- kubectl create deployment --image=kodekloud/webapp-color webapp --replicas=3 --dry-run=client -o yaml\n",
    "\n",
    "‚ò∏Ô∏è\n",
    "- kubectl run custom-nginx --image=nginx --port=8080 --dry-run=client -o yaml\n",
    "\n",
    "‚ò∏Ô∏è Create a new namespace called dev-ns.\n",
    "- kubectl create namespace dev-ns\n",
    "\n",
    "‚ò∏Ô∏è Create a pod named httpd using the image httpd:alpine in the default namespace.\n",
    "Then, create a service of type ClusterIP with the same name (httpd) that exposes the pod on port 80.\n",
    "- kubectl expose pod httpd --port=80 --name=httpd --type=ClusterIP\n",
    "\n",
    "‚ò∏Ô∏è Which command would you use to display a list of all available API resources in your cluster?\n",
    "- kubectl api-resources\n",
    "\n",
    "‚ò∏Ô∏è What is the SHORTNAME for horizontalpodautoscalers? This shortname can be used instead of typing the full resource name in kubectl commands.\n",
    "-hpa\n",
    "\n",
    "‚ò∏Ô∏è Which of the following resources is NOT namespaced (i.e., cluster-scoped)?\n",
    "- nodes\n",
    "\n",
    "‚ò∏Ô∏è Which command would you use to get a description and details of the Pod resource?\n",
    "- Kubectl explain pod\n",
    "\n",
    "‚ò∏Ô∏è When writing a YAML manifest for a Pod, you want to understand what fields are available under containers. \n",
    "Which command would you use to explore the structure of the containers field?\n",
    "- kubectl explain pod.spec.containers\n",
    "\n",
    "‚ò∏Ô∏è You can drill down into nested fields using dot notation. Use kubectl explain to explore the spec field of a Pod. What is the TYPE of the containers field?\n",
    "== [] object\n",
    "\n",
    "‚ò∏Ô∏è Let's explore the Deployment resource. What is the TYPE of the replicas field?\n",
    "- integer\n",
    "\n",
    "‚ò∏Ô∏è What does the --recursive flag do when used with kubectl explain?\n",
    "- The --recursive flag makes kubectl explain print all the fields of a resource at once, including all of its nested fields.\n",
    "\n",
    "‚ò∏Ô∏è Use kubectl explain with the --recursive flag to explore the service.spec.ports structure. (kubectl explain --recursive service.spec.ports)\n",
    "Looking at the recursive output, which of these fields is not available under service.spec.ports?\n",
    "- hostNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ad241",
   "metadata": {},
   "source": [
    "10/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Why is the POD in a pending state? Check the below points\n",
    "============================================================\n",
    "\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Describe the Pod\n",
    "- kubectl describe pod <pod-name> -n <namespace>\n",
    "\n",
    "‚ò∏Ô∏è Check the Pod Events Only\n",
    "- kubectl get events --sort-by=.lastTimestamp -n <namespace>\n",
    "\n",
    "‚ò∏Ô∏è Check if Nodes Have Resources\n",
    "- kubectl get nodes\n",
    "- kubectl describe node <node-name>\n",
    "\n",
    "‚ò∏Ô∏è Check If It‚Äôs Waiting for a PVC\n",
    "- kubectl get pvc -n <namespace>\n",
    "\n",
    "‚ò∏Ô∏è If Using a Custom Image\n",
    "- kubectl describe pod <pod-name> | grep -i image\n",
    "\n",
    "‚ò∏Ô∏è If Node Affinity / Taints Are Involved\n",
    "- kubectl describe pod <pod-name> | grep -i tolerations -A5\n",
    "- kubectl describe node <node-name> | grep -i taints\n",
    "\n",
    "\n",
    "============================================================\n",
    "\n",
    "‚ò∏Ô∏è how to manially asign node in yaml file?\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "spec:\n",
    "  nodeName: node1    # <-- This is the correct key\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "\n",
    "\n",
    "============================================================\n",
    "\n",
    "# ‚ò∏Ô∏è Annotataion in k8s\n",
    "\n",
    "- Annotations in Kubernetes are like sticky notes you attach to objects. They don‚Äôt affect how the object behaves, but they store extra information that tools and humans might care about later.\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: nginx\n",
    "  annotations:\n",
    "    owner: \"pratul\"\n",
    "    purpose: \"demo-pod\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "\n",
    "‚ò∏Ô∏è Labels vs Annotations\n",
    "\n",
    "| Feature                              | Label      | Annotation              |\n",
    "| ------------------------------------ | ---------- | ----------------------- |\n",
    "| Used for selecting/filtering         | ‚úÖ Yes      | ‚ùå No                    |\n",
    "| Short and structured data            | ‚úÖ Yes      | ‚ùå No (can be long text) |\n",
    "| Used by schedulers/controllers       | ‚úÖ Yes      | ‚ùå No                    |\n",
    "| Used to store external/tool metadata | ‚ö†Ô∏è Limited | ‚úÖ Yes                   |\n",
    "\n",
    "\n",
    "# ‚ò∏Ô∏è Labels\n",
    "- kubectl get pods --selector env=dev\n",
    "\n",
    "‚û°Ô∏è How many objects are in the prod environment including PODs, ReplicaSets and any other objects?\n",
    "- kubectl get all --selector env=prod\n",
    "\n",
    "‚û°Ô∏è Identify the POD which is part of the prod environment, the finance BU and of frontend tier?\n",
    "- kubectl get pods -l \"env=prod,bu=finance,tier=frontend\"\n",
    "\n",
    " # ‚ò∏Ô∏è  Taints and Tolerations\n",
    "\n",
    "‚û°Ô∏è Taints are set on Nodes and Tolerations are set on Pods. Taints & Tolerations doesn't tell a pod to go to a particular node Instead it tell the node to accept pods with certain toleration. \n",
    "\n",
    "‚û°Ô∏è To restrict a pods to certain nodes it is achieved through concept \"node-affinity\" \n",
    "\n",
    "‚û°Ô∏è Quick Comparison\n",
    "\n",
    "| Taint Effect         | Blocks new Pods?               | Evicts existing Pods? |\n",
    "| -------------------- | ------------------------------ | --------------------- |\n",
    "| **NoSchedule**       | Yes                            | No                    |\n",
    "| **PreferNoSchedule** | Tries to avoid, but not strict | No                    |\n",
    "| **NoExecute**        | Yes                            | Yes                   |\n",
    "\n",
    "\n",
    "‚û°Ô∏è A taint is applied on the master node to prevent pods scheduling on it.\n",
    "- kubectl describe nodes kubemaster  | grep Taint \n",
    "\n",
    "‚û°Ô∏è Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule\n",
    "- kubectl taint nodes node01 spray=mortein:NoSchedule\n",
    "\n",
    "‚û°Ô∏è Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.\n",
    "- Change the Yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: bee\n",
    "spec:\n",
    "  tolerations:\n",
    "  - key: \"spray\"\n",
    "    operator: \"Equal\"\n",
    "    value: \"mortein\"\n",
    "    effect: \"NoSchedule\"\n",
    "  containers:\n",
    "  - name: bee\n",
    "    image: nginx\n",
    "\n",
    "\n",
    "‚û°Ô∏è Remove the taint on controlplane, which currently has the taint effect of NoSchedule. (Just add \"-\")\n",
    "- kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-\n",
    "\n",
    "‚û°Ô∏è check \n",
    "- kubectl describe node controlplane | grep Taint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c6aa7",
   "metadata": {},
   "source": [
    "17/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Node Affinity\n",
    "\n",
    "- Purpose: Schedule pods preferentially or restrictively to specific nodes based on labels.\n",
    "- How it works: You define rules in your Pod spec that match node labels.\n",
    "\n",
    "‚û°Ô∏è Types:\n",
    "\n",
    "- requiredDuringSchedulingIgnoredDuringExecution ‚Üí Mandatory: Pod won‚Äôt schedule unless the node matches.\n",
    "- preferredDuringSchedulingIgnoredDuringExecution ‚Üí Best effort: Scheduler tries, but may ignore it if needed.\n",
    "\n",
    "\n",
    "affinity:\n",
    "  nodeAffinity:\n",
    "    requiredDuringSchedulingIgnoredDuringExecution:\n",
    "      nodeSelectorTerms:\n",
    "      - matchExpressions:\n",
    "        - key: role\n",
    "          operator: In\n",
    "          values:\n",
    "          - backend\n",
    "\n",
    "\n",
    "‚ò∏Ô∏è Node Affinity vs Taints and tolerations\n",
    "\n",
    "| Feature          | Node Affinity                               | Taints & Tolerations                                     |\n",
    "| ---------------- | ------------------------------------------- | -------------------------------------------------------- |\n",
    "| Direction        | Pod **requests** nodes with matching labels | Node **repels** pods unless allowed                      |\n",
    "| Scope            | Scheduling rule only                        | Applies both during scheduling and execution (NoExecute) |\n",
    "| Role             | \"Let me run **on** certain nodes\"           | \"Keep me **off** certain nodes\"                          |\n",
    "| Labels vs Taints | Uses node labels                            | Uses taints on nodes and tolerations on pods             |\n",
    "\n",
    "\n",
    "‚û°Ô∏è How many Labels exist on node node01?\n",
    "- kubectl describe  node node01 \n",
    "\n",
    "‚û°Ô∏è Apply a label color=blue to node node01\n",
    "- kubectl label node node01 color=blue\n",
    "\n",
    "‚û°Ô∏è how to select all labels from below command\n",
    "- kubectl get node node01 --show-labels                    or\n",
    "- kubectl get node node01 -o json | jq -r '.metadata.labels | to_entries[] | \"\\(.key)=\\(.value)\"'\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "‚û°Ô∏è Edit a POD\n",
    "- kubectl get pod webapp -o yaml > my-new-pod.yaml\n",
    "\n",
    "Then make the changes to the exported file using an editor (vi editor). Save the changes\n",
    "- vi my-new-pod.yaml\n",
    " \n",
    "Then delete the existing pod\n",
    "- kubectl delete pod webapp\n",
    "\n",
    "Then create a new pod with the edited file\n",
    "- kubectl create -f my-new-pod.yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b3845",
   "metadata": {},
   "source": [
    "19/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è DaemonSets\n",
    "\n",
    "‚û°Ô∏è A DaemonSet is a Kubernetes workload object (like a Deployment) but with a very specific rule: It ensures that a copy of a Pod is running on all (or some) Nodes in the cluster. If you add a new Node to your cluster, the DaemonSet automatically spins up a Pod on it. If you remove a Node, that Pod is garbage collected.\n",
    "\n",
    "‚û°Ô∏è One of the use case of daemon set is for Monitoring Solution or Logs Viewers. Kube Proxy can also be deployed as DaemonSets in k8s cluster. \n",
    "‚û°Ô∏è Another use case is networking sloutions like calico. \n",
    "\n",
    "‚û°Ô∏è How many DaemonSets are created in the cluster in all namespaces?\n",
    "- kubectl get daemonsets --all-namespaces     or\n",
    "- kubectl get daemonsets -A\n",
    "\n",
    "‚û°Ô∏è To check in a particular namespace.\n",
    "- kubectl get daemonsets -n yoyo\n",
    "\n",
    "‚û°Ô∏è Below Output explained.\n",
    "\n",
    "kubectl get daemonsets -A\n",
    "NAMESPACE      NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
    "kube-flannel   kube-flannel-ds   1         1         1       1            1           <none>                   13m\n",
    "kube-system    kube-proxy        1         1         1       1            1           kubernetes.io/os=linux   13m\n",
    "\n",
    "Column\tValue\tMeaning in your specific case\n",
    "DESIRED\t1\tYou have 1 eligible node (your machine), so k8s wants 1 pod.\n",
    "CURRENT\t1\tThat 1 pod has been successfully created on the node.\n",
    "READY\t1\tThat 1 pod is healthy and passing health checks.\n",
    "UP-TO-DATE\t1\tThat 1 pod is using the latest configuration (you aren't currently updating it).\n",
    "AVAILABLE\t1\tThat 1 pod is stable and usable.\n",
    "\n",
    "‚û°Ô∏è What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?\n",
    "- kubectl describe daemonset kube-flannel-ds -n kube-flannel\n",
    "\n",
    "‚û°Ô∏è Deploy a DaemonSet for FluentD Logging. Name: elasticsearch, Namespace: kube-system, Image: registry.k8s.io/fluentd-elasticsearch:1.20   (Shortcut)\n",
    "\n",
    "Step 1: Generate the YAML file Run this command to create a file named ds.yaml:\n",
    "- kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > ds.yaml\n",
    "\n",
    "Step 2: Edit the file Open the file (vi ds.yaml or nano ds.yaml) and make these two small changes:\n",
    "- Change kind: Deployment to kind: DaemonSet.\n",
    "- Delete the line replicas: 1 (DaemonSets do not use replica counts).\n",
    "- (Optional) Delete the strategy: {} section if present.\n",
    "\n",
    "Step 3: Apply it\n",
    "\n",
    "-------------------------------------------------------------------------------------------\n",
    "\n",
    "‚ò∏Ô∏è Static Pods\n",
    "---------------\n",
    "\n",
    "üëâ Static Pods are special Kubernetes pods that are created and managed directly by the Kubelet, not by the Kubernetes control plane (API Server, Scheduler, Controller Manager).\n",
    "\n",
    "‚û°Ô∏è Static Pod ‚Äì Key Use Cases\n",
    "Running Kubernetes Control Plane Components\n",
    "Static Pods are most commonly used to run:\n",
    "\n",
    "- kube-apiserver\n",
    "- kube-controller-manager\n",
    "- kube-scheduler\n",
    "- etcd (in kubeadm setups)\n",
    "\n",
    "üëâ These are critical services that must start even before the Kubernetes API server is available.\n",
    "üëâ Using staticpods ensures the control plane comes up automatically as long as kubelet is running.\n",
    "\n",
    "‚û°Ô∏è Staticpod vs deamonsets\n",
    "\n",
    "| Feature / Aspect           | Static Pod                                           | DaemonSet                                                 |\n",
    "|----------------------------|-------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Who creates the pod?**  | Kubelet directly (no API server involved)             | Kubernetes API Server via Controller Manager              |\n",
    "| **Where is it defined?**  | Manifest file on each node (e.g., /etc/kubernetes/manifests/) | YAML manifest applied to the cluster (kubectl apply)      |\n",
    "                              or /var/lib/kubelet/config.yaml\n",
    "| **API object exists?**    | No Pod object exists in ETCD (mirror pod is created)  | Yes, DaemonSet object stored in ETCD                      |\n",
    "| **Automatic rescheduling**| Yes, but only by kubelet on the same node             | Yes across the cluster by DaemonSet controller            |\n",
    "| **Pod placement**         | Only on the node where the manifest file exists       | On all or selected nodes using labels, nodeSelectors, taints/tolerations |\n",
    "| **Use case**              | Critical system components (kube-apiserver, etcd)     | Logging agents, monitoring agents, network plugins        |\n",
    "| **Updates / Changes**     | Manual update per node                                | Rolling updates supported                                 |\n",
    "| **Deletion method**       | Delete the file on the node; kubelet kills the pod    | kubectl delete daemonset <name>                           |\n",
    "| **Managed by Kubernetes?**| Partially (only kubelet controls it)                  | Fully managed by Kubernetes                               |\n",
    "| **Scaling**               | Not supported (one per manifest per node)             | Automatically runs one pod per node                       |\n",
    "| **Node failure behavior** | Pod recreated only when node comes back               | Pod scheduled on new healthy nodes                        |\n",
    "| **Logging/Monitoring**    | Harder (as not fully in API server)                   | Fully integrable with cluster observability tools         |\n",
    "                            |\n",
    "\n",
    "üëâ How many static pods exist in this cluster in all namespaces? \n",
    "- (Hint - Run the command kubectl get pods --all-namespaces and look for those with -<Node-Name> appended in the name at end?)\n",
    "\n",
    "üëâ Which of the below components is NOT deployed as a static pod?\n",
    "(- (Hint - Run kubectl get pods --all-namespaces and look for the pod from the list that does not end with -<Node-Name>)\n",
    "\n",
    "üëâ On which nodes are the static pods created currently?\n",
    "- kubectl get pods --all-namespaces -o wid\n",
    "\n",
    "üëâ What is the docker image used to deploy the kube-api server as a static pod?\n",
    "- kubectl describe pod kube-apiserver-controlplane -n kube-system | grep Image\n",
    "\n",
    "üëâ Create a static pod named static-busybox that uses the busybox image , run in the default namespace and the command sleep 1000\n",
    "- Create a pod definition file called static-busybox.yaml with the provided specs and place it under /etc/kubernetes/manifests directory.\n",
    "\n",
    "üëâ We just created a new static pod named static-greenbox. Find it and delete it.\n",
    "\n",
    "- Identify which node the static pod is created on, ssh to the node and delete the pod definition file. (kubectl get pods --all-namespaces)\n",
    "- If you don't know the IP of the node, run the kubectl get nodes -o wide command and identify the IP. (kubectl describe pod pod-name) then >> ssh root@192.168.59.143\n",
    "- Then, SSH to the node using that IP. For static pod manifest path look at the file /var/lib/kubelet/config.yaml on node01.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d4f8b",
   "metadata": {},
   "source": [
    "19/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è PriorityClass\n",
    "-------------------\n",
    "\n",
    "‚û°Ô∏è A PriorityClass defines the importance of pods. Higher-priority pods are scheduled first and can evict lower-priority pods when the cluster is low on resources.\n",
    "\n",
    "‚û°Ô∏è It helps in:\n",
    "\n",
    "- Scheduling critical workloads first\n",
    "- Ensuring system pods always run\n",
    "- Defining environments like ‚Äúhigh‚Äù, ‚Äúmedium‚Äù, ‚Äúlow‚Äù priority\n",
    "\n",
    "‚û°Ô∏è How Priority Works\n",
    "\n",
    "- Priority is a number (value field).\n",
    "- Higher number = Higher priority.\n",
    "- A pod with priority 100000 is scheduled before a pod with 50000.\n",
    "- A very high priority pod can evict low-priority pods under resource pressure (Preemption)\n",
    "\n",
    "\n",
    "apiVersion: scheduling.k8s.io/v1\n",
    "kind: PriorityClass\n",
    "metadata:\n",
    "  name: high-priority\n",
    "value: 100000\n",
    "globalDefault: false\n",
    "preemptionPolicy: PreemptLowerPriority\n",
    "description: testing testing 123..\n",
    "\n",
    "‚û°Ô∏è What is Preemption in Kubernetes?\n",
    "- Preemption is a mechanism where Kubernetes evicts lower-priority pods to make room for a higher-priority pod when the cluster is out of resources (CPU, RAM).\n",
    "\n",
    "HIGH priority pod arrives ‚Üí cluster has no space ‚Üí Kubernetes removes LOW priority pod(s) ‚Üí frees space ‚Üí schedules the high priority pod.\n",
    "\n",
    "‚û°Ô∏è Types of PREEMPTIONPOLICY\n",
    "- PreemptLowerPriority\n",
    "- Never\n",
    "- disable preemption for specific pods (using preemptionPolicy: Never\n",
    "\n",
    "üëâ Which of the following PriorityClasses are part of a default Kubernetes setup?\n",
    "- kubectl get priorityclass\n",
    "\n",
    "üëâ What is the priority value assigned to system-node-critical?\n",
    "- kubectl describe priorityClass system-node-critical \n",
    "\n",
    "üëâ To know about a particular priorityClass.\n",
    "- kubectl get priorityClass system-node-critical \n",
    "NAME                   VALUE        GLOBAL-DEFAULT   AGE   PREEMPTIONPOLICY\n",
    "system-node-critical   2000001000   false            19m   PreemptLowerPriority\n",
    "\n",
    "üëâ To assign a priority to pod.\n",
    "- low-prio-pod.yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: low-prio-pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx\n",
    "  priorityClassName: low-priority\n",
    "\n",
    "\n",
    "üëâ You can compare the priority classes on both pods using the following command:\n",
    "- kubectl get pods -o custom-columns=\"NAME:.metadata.name,PRIORITY:.spec.priorityClassName\"\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "üëâ To see log\n",
    "- kubectl get events -o wide\n",
    "\n",
    "üëâ What is the name of the POD that deploys the default kubernetes scheduler in this environment?\n",
    "- kubectl get pods --namespace=kube-system \n",
    "\n",
    "üëâ What is the image used to deploy the kubernetes scheduler?\n",
    "- kubectl describe pod kube-scheduler-controlplane -n kube-system | grep Image\n",
    "\n",
    "üëâ Please modify the provided Pod manifest file located at /root/nginx-pod.yaml to specify that the Pod should be scheduled by your custom scheduler, which is named my-scheduler. After updating, create the Pod in the default namespace and verify it is scheduled by your custom scheduler.\n",
    "\n",
    "----\n",
    "apiVersion: v1 \n",
    "kind: Pod \n",
    "metadata:\n",
    "  name: nginx \n",
    "spec:\n",
    "  schedulerName: my-scheduler\n",
    "  containers:\n",
    "  - image: nginx\n",
    "    name: nginx\n",
    "\n",
    "üëâ \n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5ea71",
   "metadata": {},
   "source": [
    "\n",
    "25/11/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Admission Controller \n",
    "\n",
    "‚û°Ô∏è Think of admission controller as the final, strict security checkpoint at a high-security facility.\n",
    "Even if you have a valid ID badge (Authentication) and you are allowed to be in that specific building (Authorization), the security guard still stops you to check what you are carrying. They might force you to wear a safety helmet (Mutation) or stop you because you are carrying a prohibited item (Validation).\n",
    "\n",
    "‚û°Ô∏è They are used to enforce policies, validate configurations, and perform additional operations on resources being created or modified. However, they do not handle user authentication.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "üëâ What is not a function of admission controller?\n",
    "- authenticate user\n",
    "\n",
    "üëâ Which admission controller is not enabled by default? (You can check the default enabled admission controllers by running the following command inside the control plane node or pod:)\n",
    "- kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'\n",
    "\n",
    "üëâ Which admission controller is enabled in this cluster which is normally disabled?  (To check for the enabled admission plugins in the Kubernetes API server configuration, execute the following command:)\n",
    "- grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml\n",
    "\n",
    "üëâ The previous step failed due to the NamespaceExists admission controller being enabled in Kubernetes. This controller rejects requests for namespaces that do not already exist. Therefore, to automatically create a namespace that does not exist, you should enable the NamespaceAutoProvision admission controller. Enable the NamespaceAutoProvision admission controller.\n",
    "\n",
    "- Add NamespaceAutoProvision admission controller to the --enable-admission-plugins list\n",
    "in /etc/kubernetes/manifests/kube-apiserver.yaml.\n",
    "\n",
    "It should look like this:\n",
    "spec:\n",
    "  containers:\n",
    "    - name: kube-apiserver\n",
    "      image: registry.k8s.io/kube-apiserver:v1.33.0\n",
    "      imagePullPolicy: IfNotPresent\n",
    "      command:\n",
    "        - kube-apiserver\n",
    "        - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision\n",
    "        # ...other flags...\n",
    "\n",
    "After saving the file, the API server will automatically restart and pick up this configuration.\n",
    "Check pod status with:\n",
    "kubectl get pods -n kube-system\n",
    "\n",
    "Note: This command may not yield immediate results due to the updated configuration of the kube-apiserver. please wait for a few minutes for the kube-apiserver to restart completely.\n",
    "\n",
    "üëâ To see pvc\n",
    "- kubectl get pvc\n",
    "\n",
    "üëâ Which of the below combination is correct for Mutating and validating admission controllers ?\n",
    "- ‚úî MutatingAdmissionWebhook & ‚úî ValidatingAdmissionWebhook\n",
    "\n",
    "üëâ What is the flow of invocation of admission controllers?\n",
    "- M ‚Üí V ‚Üí S (Mutate ‚Üí Validate ‚Üí Store)\n",
    "\n",
    "üëâ Create namespace webhook-demo where we will deploy webhook components\n",
    "- kubectl create namespace webhook-demo\n",
    "\n",
    "üëâ Create a TLS secret named webhook-server-tls in the webhook-demo namespace. This secret will be used by the admission webhook server for secure communication over HTTPS.\n",
    "We have already created below cert and key for webhook server which should be used to create secret.\n",
    "\n",
    "Certificate : /root/keys/webhook-server-tls.crt\n",
    "Key : /root/keys/webhook-server-tls.key\n",
    "\n",
    "- kubectl -n webhook-demo create secret tls webhook-server-tls \\\n",
    "    --cert \"/root/keys/webhook-server-tls.crt\" \\\n",
    "    --key \"/root/keys/webhook-server-tls.key\"\n",
    "\n",
    "üëâ Create the webhook deployment that will run the admission webhook server. We have already provided the deployment manifest at: /root/webhook-deployment.yaml\n",
    "Create the deployement using this definition.\n",
    "-  kubectl create -f /root/webhook-deployment.yaml\n",
    "\n",
    "üëâ Create a service that exposes the webhook server so that the admission controller can communicate with it. We have already provided the service manifest at: /root/webhook-service.yaml\n",
    "Create the service using this definition.\n",
    "-  kubectl create -f /root/webhook-service.yaml\n",
    "\n",
    "üëâ We have added the MutatingWebhookConfiguration under /root/webhook-configuration.yaml. Upon applying this configuration, which resources and actions will it impact?\n",
    "- POD with CREATE option\n",
    "\n",
    "üëâ Now lets deploy MutatingWebhookConfiguration in /root/webhook-configuration.yaml\n",
    "-  kubectl create -f /root/webhook-configuration.yaml\n",
    "\n",
    "üëâ Check the securityContext of the pod created in the previous step (pod-with-defaults). Even though we did not specify any values in the pod definition, the mutation webhook should have injected default values.\n",
    "\n",
    "Execute the following command to retrieve the security context settings for the specified pod:\n",
    "- kubectl get pod pod-with-defaults -o yaml | grep -A2 \"securityContext:\"\n",
    "\n",
    "üëâ what is this A2 in below command kubectl get po pod-with-override -o yaml | grep -A2 \" securityContext:\"\n",
    "-A stands for ‚ÄúAfter‚Äù\n",
    "-A2 means:\n",
    "‚û°Ô∏è Show 2 lines After the matching line.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd8369",
   "metadata": {},
   "source": [
    "08/12/2025\n",
    "----------\n",
    "\n",
    "‚ò∏Ô∏è Logging & Monitoring\n",
    "\n",
    "> We can have 1 metric server per K8s cluster. Heapster is depricated. Metric server is in memory monitoring sol. does not store in disk.\n",
    "> CAdvisor is responsible for retriving perfromance metrics from pod and exposing them through the kublet api to make metrics available for the metric server.\n",
    "> To get metric details of nodes.\n",
    "\n",
    "kubectl top node\n",
    "NAME           CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   \n",
    "controlplane   307m         1%       914Mi           1%          \n",
    "node01         30m          0%       170Mi           0% \n",
    "\n",
    "> To check logs of single conatiner in a pod. (-f is used to stream live logs.)\n",
    "- kubectl logs -f <POD-NAME>\n",
    "\n",
    "> To check logs of multiple conatiner in a pod. (Always specify container name in case pod has multiple containers running in it.)\n",
    "- kubectl logs - f <POD-NAME> <CONTAINER-NAME>\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "‚ò∏Ô∏è Application Lifecycle management.\n",
    "--------------------------------------\n",
    "\n",
    "‚û°Ô∏è Rollouts\n",
    "------------\n",
    "\n",
    "> To check the status of a rollout\n",
    "- kubectl rollout status deployment/<DEPLOYMENT-NAME>\n",
    "\n",
    "> To see the revision/history of rollouts.\n",
    "- kubectl rollout history deployment/<DEPLOYMENT-NAME>\n",
    "\n",
    "> To undo a rollut\n",
    "- kubectl rollout undo  deployment <DEPLOYMENT-NAME>\n",
    "\n",
    "‚û°Ô∏è Deployment strategy\n",
    "-----------------------\n",
    "1. Recreate      (All previous pods and down and then new version pods are up. Problem with this is that application dowtime.)\n",
    "2. Rolling Update (Default)\n",
    "\n",
    "> To check Strategy Type of a deployemnt.\n",
    "- kubectl describe deployment frontend | grep StrategyType\n",
    "\n",
    "> Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2 \n",
    "(Do not delete and re-create the deployment. Only set the new image name for the existing deployment.)\n",
    "- Run the command \"kubectl edit deployment frontend\" and modify the image to kodekloud/webapp-color:v2. Next, save and exit. The pods should be recreated with the new image.\n",
    "\n",
    "> Change the deployment strategy to Recreate\n",
    "- Run the command kubectl edit deployment frontend and modify the required field. Make sure to delete the properties of rollingUpdate as well, set at spec.strategy.rollingUpdate.\n",
    "\n",
    "\n",
    "# Configuring applications comprises of understanding the following concepts:\n",
    "\n",
    "- Configuring Command and Arguments on applications\n",
    "- Configuring Environment Variables\n",
    "- Configuring Secrets\n",
    "\n",
    "\n",
    "09/12/2025\n",
    "----------\n",
    "\n",
    "> What is the command used to run the pod ubuntu-sleeper?\n",
    "- kubectl describe pod ubuntu-sleeper      and >>> check \"Command:\" section.\n",
    "\n",
    "> Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.\n",
    "---\n",
    "apiVersion: v1 \n",
    "kind: Pod \n",
    "metadata:\n",
    "  name: ubuntu-sleeper-2 \n",
    "spec:\n",
    "  containers:\n",
    "  - name: ubuntu\n",
    "    image: ubuntu\n",
    "    command: ['sleep', '5000']\n",
    "\n",
    "> You are given a directory webapp-color-3 that contains a Dockerfile and a Kubernetes pod YAML file webapp-color-pod-2.yaml. When the Pod defined in webapp-color-pod-2.yaml starts, which command will actually run inside the container?\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod \n",
    "metadata:\n",
    "  name: webapp-green\n",
    "  labels:\n",
    "      name: webapp-green\n",
    "spec:\n",
    "  containers:\n",
    "  - name: simple-webapp\n",
    "    image: kodekloud/webapp-color\n",
    "    command: [\"python\", \"app.py\"]\n",
    "    args: [\"--color\", \"pink\"]\n",
    "\n",
    "- python app.py  --color pink\n",
    "\n",
    "> Create a Kubernetes Pod that runs a web application with a green background.\n",
    "Requirements:\n",
    "- Pod name: webapp-green\n",
    "- Docker image: kodekloud/webapp-color\n",
    "- The app should display a green background (not the default blue)\n",
    "- Use the command line argument: --color=green\n",
    "\n",
    "apiVersion: v1 \n",
    "kind: Pod \n",
    "metadata:\n",
    "  name: webapp-green\n",
    "  labels:\n",
    "      name: webapp-green \n",
    "spec:\n",
    "  containers:\n",
    "  - name: simple-webapp\n",
    "    image: kodekloud/webapp-color\n",
    "    args: [\"--color=green\"]\n",
    "\n",
    "\n",
    "‚úÖ Correct logic\n",
    "Use args when you want to pass parameters to the default command (ENTRYPOINT).\n",
    "Use command when you want to override the default command.\n",
    "\n",
    "| Goal                                               | Use       |\n",
    "| -------------------------------------------------- | --------- |\n",
    "| Run the image's default command with new arguments | `args`    |\n",
    "| Replace the image's default command completely     | `command` |\n",
    "| Run a totally custom script or binary              | `command` |\n",
    "| Add options to existing ENTRYPOINT                 | `args`    |\n",
    "\n",
    ">  What is the environment variable name set on the container in the pod?\n",
    "-  kubectl describe pod webapp-color and check section \"Environment:\"\n",
    "\n",
    "> Update the environment variable on the POD to display a green background. Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.\n",
    "\n",
    "- kubectl delete pod webapp-color --ignore-not-found && kubectl run webapp-color --image=kodekloud/webapp-color -l name=webapp-color --env APP_COLOR=green --restart=Never \n",
    "\n",
    "\n",
    "# ConfigMap \n",
    "(Imperative way)\n",
    "- kubectl create configmap <config-name> --from-literal=APP_COLOR=blue  --from-literal=APP_MOD=PROD                or      \n",
    "- kubectl create configmap <config-name> --from-file=<path-to-file>\n",
    "\n",
    "(Declarative way)\n",
    "\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: game-demo\n",
    "data:\n",
    "  # property-like keys; each key maps to a simple value\n",
    "  player_initial_lives: \"3\"\n",
    "  ui_properties_file_name: \"user-interface.properties\"\n",
    "\n",
    "  # file-like keys\n",
    "  game.properties: |\n",
    "    enemy.types=aliens,monsters\n",
    "    player.maximum-lives=5    \n",
    "  user-interface.properties: |\n",
    "    color.good=purple\n",
    "    color.bad=yellow\n",
    "    allow.textmode=true \n",
    "\n",
    "09/12/2025\n",
    "----------\n",
    "\n",
    "> Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap. Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.\n",
    "-\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  labels:\n",
    "    name: webapp-color\n",
    "  name: webapp-color\n",
    "  namespace: default\n",
    "spec:\n",
    "  containers:\n",
    "  - env:\n",
    "    - name: APP_COLOR\n",
    "      valueFrom:\n",
    "       configMapKeyRef:\n",
    "         name: webapp-config-map\n",
    "         key: APP_COLOR\n",
    "    image: kodekloud/webapp-color\n",
    "    name: webapp-color\n",
    "\n",
    "# ConfigMap \n",
    "\n",
    "- Imperative way\n",
    "kubectl create secret generic <secret-name> --from-literal=<key>=<value>\n",
    "kubectl create secret generic db-secret --from-literal=DB_Host=sql01  --from-literal=DB_User=root --from-literal=DB_Password=password123\n",
    "or\n",
    "kubectl create secret generic <secret-name> --from-file=<file-path>\n",
    "\n",
    "- Declarative way\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: test-secret\n",
    "data:\n",
    "  username: bXktYXBw\n",
    "  password: Mzk1MjgkdmRnN0pi\n",
    "\n",
    "# use secret in pod creation yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: env-single-secret\n",
    "spec:\n",
    "  containers:\n",
    "  - name: envars-test-container\n",
    "    image: nginx\n",
    "    env:\n",
    "    - name: SECRET_USERNAME\n",
    "      valueFrom:\n",
    "        secretKeyRef:\n",
    "          name: backend-user\n",
    "          key: backend-username\n",
    "\n",
    "\n",
    "> To view a secret in yaml format\n",
    "-kubectl get secret <secret-name> -o yaml\n",
    "\n",
    "> What is the type of the dashboard-token secret?\n",
    "- kubectl describe secrets dashboard-token \n",
    "\n",
    "> Configure webapp-pod to load environment variables from the newly created secret. Note: If the pod already exists, you must delete and recreate it. Updating a pod's environment variables (like envFrom) is not allowed on a running pod.\n",
    "- \n",
    "---\n",
    "apiVersion: v1 \n",
    "kind: Pod \n",
    "metadata:\n",
    "  labels:\n",
    "    name: webapp-pod\n",
    "  name: webapp-pod\n",
    "  namespace: default \n",
    "spec:\n",
    "  containers:\n",
    "  - image: kodekloud/simple-webapp-mysql\n",
    "    imagePullPolicy: Always\n",
    "    name: webapp\n",
    "    envFrom:\n",
    "    - secretRef:\n",
    "        name: db-secret\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "13/12/2025\n",
    "----------\n",
    "\n",
    "# Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. \n",
    "https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/\n",
    "\n",
    "# Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault.\n",
    "\n",
    "Helm Secrets (often called helm-secrets) is a popular plugin for Helm that allows you to encrypt and decrypt sensitive values (like passwords, API keys, certificates) inside Helm charts using tools like Mozilla SOPS and GPG.\n",
    "\n",
    "In simple terms:\n",
    "üöÄ Helm Secrets = Secure way to manage secrets in Helm charts\n",
    "\n",
    "Normally, Helm requires you to put secrets in values.yaml, which is insecure if you're storing the file in Git.\n",
    "Helm Secrets solves this by letting you keep an encrypted file (values.secret.yaml) in Git and decrypt it only when installing/upgrading the Helm release.\n",
    "\n",
    "\n",
    "> To decide the secert use below cmd.\n",
    "- echo \"=onnfj172\" | base64 --decode\n",
    "\n",
    "> After encryption is enabled only things that you create after that will be encrypted, everything that existed previosly won't be encrypted. But if you updated an existing configuration that will be encrypted.\n",
    "\n",
    "> MUlti-container pods share same network means they can referto each other through local host. \n",
    "\n",
    "> Design patterns of multi-containers pods.\n",
    "1. Co-located continers (Both run side-by-side through the life cycle of pod.)\n",
    "2. Regular Init Containers (It starts before the main container and stops when its proess is complete, if it fail te pods restarts untill init container is complete)\n",
    "3. SideCar Containers  (Same as init containers it starts first before main container and instead of stopping it continues to run throughout the life cycle of pod and\n",
    "it end after the main app container stops.)\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "25/12/2025\n",
    "----------\n",
    "> init Containers.\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: myapp-pod\n",
    "  labels:\n",
    "    app: myapp\n",
    "spec:\n",
    "  containers:\n",
    "  - name: myapp-container\n",
    "    image: busybox:1.28\n",
    "    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n",
    "  initContainers:\n",
    "  - name: init-myservice\n",
    "    image: busybox:1.28\n",
    "    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n",
    "  - name: init-mydb\n",
    "    image: busybox:1.28\n",
    "    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']ii\n",
    "\n",
    "\n",
    "> Why is the initContainer terminated? What is the reason?\n",
    "- The Process Completed Successfully.\n",
    "\n",
    "> How long after the creation of the purple POD will the application come up and be available to users?\n",
    "- kubectl describe pod purple \n",
    "(Check the commands used in the initContainers. The first one sleeps for 600 seconds (10 minutes) and the second one sleeps for 1200 seconds (20 minutes))\n",
    "\n",
    "> A new application orange is deployed. There is something wrong with it. Identify and fix the issue. Once fixed, wait for the application to run before checking solution.\n",
    "1. Export the pod's YAML definition to a file:\n",
    "- kubectl get pod orange -o yaml > orange.yaml\n",
    "\n",
    "2. Open the orange.yaml file in a text editor.\n",
    "Locate the initContainers section and find the command field. Correct the typo:\n",
    "\n",
    "command:\n",
    "- sh\n",
    "- -c\n",
    "- sleep 2\n",
    "(Change sleeeep to sleep.)\n",
    "\n",
    "3. Delete the existing faulty pod:\n",
    "- kubectl delete pod orange\n",
    "\n",
    "4. Recreate the pod using the corrected YAML file:\n",
    "- kubectl create -f orange.yaml\n",
    "\n",
    "\n",
    "‚û°Ô∏è Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. It helps in ensuring enough replicas of the application are running at all times.\n",
    "\n",
    "‚û°Ô∏è Auto Scaling\n",
    "1. Scalinig cluster Infra (Horizontal scaling - Adding more node to the cluster. Vertical Scaling - Increasing the resources i.cpu, ram etc in existing infra nodes.)\n",
    "üëâ Two Scaling\n",
    "1. Manually\n",
    "2. Automated (via. Cluster Autoscaler)\n",
    "\n",
    "2. Scaling Workload (Horizontal scaling - Creating more pods. Vertical Scaling - Increasing the resources i.cpu, ram etc allocated to  to existing pods.)\n",
    "üëâ Two Scaling\n",
    "1. Manually\n",
    "2. Automated (Horizontal pod autoscaler (HPA), Vertical pod autoscaler(VPA))\n",
    "\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  creationTimestamp: null\n",
    "  name: nginx-deployment\n",
    "spec:\n",
    "  maxReplicas: 3\n",
    "  metrics:\n",
    "  - resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        averageUtilization: 80\n",
    "        type: Utilization\n",
    "    type: Resource\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: nginx-deployment\n",
    "status:\n",
    "  currentMetrics: null\n",
    "  desiredReplicas: 0\n",
    "  currentReplicas: 0\n",
    "\n",
    "> Can the kubectl scale command be used to scale down a statefulset in Kubernetes?\n",
    "- The kubectl scale command can be used to scale both deployments and statefulsets. When scaling a statefulset, Kubernetes ensures that the state and order of the pods are maintained, unlike in deployments where pods can be created and destroyed in any order.\n",
    "\n",
    "> Manually scale the deployment named flask-web-app to have 3 replicas.\n",
    "- kubectl scale deployment flask-web-app --replicas=3\n",
    "\n",
    "> If you scale a deployment using kubectl scale to a higher number of replicas, but the cluster has insufficient resources to accommodate all new replicas, what will happen?\n",
    "- When you scale a deployment to a higher number of replicas than the cluster can support due to resource constraints, Kubernetes will create as many replicas as possible within the available resources. The remaining replicas will be in a pending state until sufficient resources are freed up or added to the cluster. This behavior allows Kubernetes to manage resources dynamically while maintaining the desired state as closely as possible.\n",
    "\n",
    "> What is the primary purpose of the Horizontal Pod Autoscaler (HPA) in Kubernetes?\n",
    "- The primary purpose of the HPA is to automatically adjust the number of pod replicas in a deployment, replication controller, or replica set based on observed metrics such as CPU utilization or custom metrics provided by the metrics server.\n",
    "\n",
    "> What component in a Kubernetes cluster is responsible for providing metrics to the HPA?\n",
    "- The metrics-server is a cluster-wide aggregator of resource usage data, and it provides the metrics required by the HPA to make scaling decisions.\n",
    "\n",
    "> What is the status of HPA target?\n",
    "- controlplane ~ ‚ûú  kubectl get hpa\n",
    "NAME               REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n",
    "nginx-deployment   Deployment/nginx-deployment   <unknown>/80%   1         3         3          11m\n",
    "\n",
    "> The HPA status shows /80 for the CPU target. what could be a possible reason?\n",
    "\n",
    "- If the status of the Horizontal Pod Autoscaler (HPA) target is <UNKNOWN>/80, it typically means that the HPA is unable to retrieve the current metrics for the specified target. Run kubectl describe hpa nginx-deployment to find more details. (The Deployment does not have any resource fields defined.)\n",
    "\n",
    "> Since the HPA was failing due to the resource field missing in the nginx-deployment, the resource field has been updated in /root/deployment.yml. Update the nginx-deployment using this manifest. Watch the changes made to the nginx-deployment by the HPA after upgrading by using the kubectl get hpa --watch command.\n",
    "\n",
    "1. update the nginx-deployment, run the below command:\n",
    "- kubectl apply -f /root/deployment.yml\n",
    "\n",
    "2. To watch the changes made to the nginx-deployment by the HPA, use the below command:\n",
    "- kubectl get hpa --watch\n",
    "\n",
    "\n",
    "> What does the event ScalingReplicaSet in the nginx-deployment HPA indicate?\n",
    "- kubectl events hpa nginx-deployment | grep -i \"ScalingReplicaSet\" (The HPA is increasing the number of pods)\n",
    "\n",
    "> What is the cause of the FailedGetResourceMetric event in the nginx-deployment HPA?\n",
    "- kubectl events hpa nginx-deployment | grep -i \"FailedGetResourceMetric\" (CPU or memery requests are missing for the resource.)\n",
    "\n",
    "\n",
    "üëâ In-Place Pod Resizing\n",
    "In-Place Pod Resizing (also known as In-Place Pod Vertical Scaling) is a Kubernetes feature that allows you to change the CPU and memory resources of a running container without restarting the Pod.\n",
    "\n",
    "Historically, any change to a Pod's resources required the Pod to be destroyed and recreated‚Äîa process that causes downtime and loss of ephemeral state. In-place resizing eliminates this by allowing the Kubelet to update the container's resource limits on the fly.\n",
    "\n",
    "\n",
    "üëâ VPA (Vertical Pod Autoscaler) components\n",
    "1. Recommender\n",
    "2. Updater\n",
    "3. Admission Controller\n",
    "\n",
    "> Which of the following are the VPA CRDs that get installed as part of the Vertical Pod Autoscaler setup?\n",
    "- kubectl get crds | grep verticalpodautoscaler\n",
    "\n",
    "verticalpodautoscalercheckpoints.autoscaling.k8s.io   2025-12-25T05:20:31Z\n",
    "verticalpodautoscalers.autoscaling.k8s.io             2025-12-25T05:20:31Z\n",
    "\n",
    "> How many VPA deployments typically run in the kube-system namespace after installation?           3\n",
    "-  kubectl get deployments -n kube-system | grep vpa\n",
    "vpa-admission-controller   1/1     1            1           2m25s\n",
    "vpa-recommender            1/1     1            1           2m26s\n",
    "vpa-updater                1/1     1            1           2m26s\n",
    "\n",
    "\n",
    "- kubectl events vpa \n",
    "- kubectl get vpa\n",
    "\n",
    "> You have recently deployed a Flask application to your Kubernetes cluster. However, the Vertical Pod Autoscaler (VPA) vpa-updater-XXXX pod indicates that there may be an issue with the newly deployed flask-app pods.\n",
    "\n",
    "- kubectl logs $(kubectl get pods -n kube-system --no-headers -o custom-columns=\":metadata.name\" | grep vpa-updater) -n kube-system\n",
    "This command first retrieves the pod name using kubectl get pods and then pipes it to the kubectl logs command to print the logs of the vpa-updater pod.\n",
    "\n",
    "\n",
    "Problem Analysis:\n",
    "\n",
    "- Flask application is running with only 1 replica pod.\n",
    "- The Vertical Pod Autoscaler (VPA) needs to evict (remove) the existing pod to create a new one with updated resource settings.\n",
    "- Kubernetes has a safety feature that prevents removing the last pod of a deployment to avoid service downtime.\n",
    "- When you have only 1 replica and VPA tries to evict it, Kubernetes blocks this action with the error message: \"too few replicas\".\n",
    "- VPA wants to optimize your pod's resources but cannot because Kubernetes is protecting your service availability.\n",
    "- As a result, VPA cannot apply its resource recommendations, and application cannot benefit from automatic resource optimization.\n",
    "\n",
    "üëâ Approach to Resolve the Issue:\n",
    "1. Increase the replica count:\n",
    "\n",
    "kubectl scale deployment flask-app --replicas=2\n",
    "\n",
    "2. Verify the Deployment:\n",
    "\n",
    "kubectl get deployment flask-app -o wide\n",
    "\n",
    "Ensure that the DESIRED column shows the updated replica count, and the CURRENT column matches the desired number.\n",
    "\n",
    "3. Check the Pod Status:\n",
    "\n",
    "kubectl get pods -l app=flask-app\n",
    "\n",
    "Wait until all pods show Running status.\n",
    "You should see two pods (or more) in a Running state.\n",
    "\n",
    "4. Verify VPA operation:\n",
    "\n",
    "kubectl describe vpa flask-app\n",
    "\n",
    "This will show the current state of the VPA and any recommendations it has made. If it's working properly, you should see resource recommendations (for CPU and memory) in the output.\n",
    "\n",
    "With 2 replicas, Kubernetes can safely remove one pod while keeping your application running, allowing VPA to work properly.\n",
    "\n",
    "\n",
    "\n",
    "> To check the current resource usage (CPU and memory) of all running pods in your Kubernetes cluster, use the \n",
    "- kubectl top pod\n",
    "\n",
    "> Initiate the load on the flask-app-4 deployment by executing the script located at /root/load.sh.\n",
    "- /root/load.sh &\n",
    "\n",
    "> Capture the recommended target CPU value from the flask-app VPA and store it in /root/target.\n",
    "\n",
    "1. Use the kubectl get command to identify the recommended target CPU value:\n",
    "-    kubectl get vpa flask-app -o yaml\n",
    "\n",
    "Look for the Recommendation section under the Status field.\n",
    "Identify the target.cpu value for the relevant container (e.g., flask-app-4).\n",
    "Manually \n",
    "\n",
    "2. Manually note the target CPU value and store it in /root/target.\n",
    "3. Store the target CPU value in /root/target:\n",
    "- echo \"143m\" > /root/target\n",
    "\n",
    "\n",
    "üëâ \n",
    "\n",
    "üëâ\n",
    "\n",
    "üëâ\n",
    "\n",
    "üëâ\n",
    "\n",
    "üëâ\n",
    "\n",
    "üëâ\n",
    "--------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
